[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
seed = 0
gpu_allocator = null

[nlp]
lang = "en"
pipeline = ["ner","llm_rel"]
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 1000
vectors = {"@vectors":"spacy.Vectors.v1"}

[components]

[components.llm_rel]
factory = "llm"
save_io = false
validate_types = true

[components.llm_rel.cache]
@llm_misc = "spacy.BatchCache.v1"
path = null
batch_size = 64
max_batches_in_mem = 4

[components.llm_rel.model]
@llm_models = "spacy.GPT-4.v1"
name = "gpt-4"
strict = true
max_tries = 5
interval = 1.0
max_request_time = 30
endpoint = null

[components.llm_rel.model.config]

[components.llm_rel.task]
@llm_tasks = "spacy.REL.v1"
labels = ["EMPLOYED_AT","PARTNERS_WITH","USES_TECH","INVESTED_IN","LOCATED_IN"]
template = "The text below contains pre-extracted entities, denoted in the following format within the text:\n{# whitespace #}\n<entity text>[ENT<entity id>:<entity label>]\n{# whitespace #}\nFrom the text below, extract the following relations between entities:\n{# whitespace #}\n{# whitespace #}\n{%- for label in labels -%}\n{{ label }}\n{# whitespace #}\n{%- endfor -%}\n{# whitespace #}\nThe extraction has to use the following format, with one line for each detected relation:\n{# whitespace #}\n{\"dep\": <entity id>, \"dest\": <entity id>, \"relation\": <relation label>}\n{# whitespace #}\nMake sure that only relevant relations are listed, and that each line is a valid JSON object.\n{# whitespace #}\n{%- if label_definitions -%}\nBelow are definitions of each label to help aid you in what kinds of relationship to extract for each label.\nAssume these definitions are written by an expert and follow them closely.\n{# whitespace #}\n{# whitespace #}\n{%- for label, definition in label_definitions.items() -%}\n{{ label }}: {{ definition }}\n{# whitespace #}\n{%- endfor -%}\n{# whitespace #}\n{# whitespace #}\n{%- endif -%}\n{%- if prompt_examples -%}\nBelow are some examples (only use these as a guide):\n{# whitespace #}\n{# whitespace #}\n{%- for example in prompt_examples -%}\nText:\n'''\n{{ preannotate(example) }}\n'''\n{# whitespace #}\n{%- for item in example.relations -%}\n{# whitespace #}\n{{ item.json() }}\n{%- endfor -%}\n{# whitespace #}\n{# whitespace #}\n{# whitespace #}\n{%- endfor -%}\n{# whitespace #}\n{# whitespace #}\n{%- endif -%}\nHere is the text that needs labeling:\n{# whitespace #}\nText:\n'''\n{{ text }}\n'''"
parse_responses = null
prompt_example_type = null
label_definitions = null
examples = null
shard_mapper = null
shard_reducer = null
normalizer = null
verbose = false

[components.ner]
source = "en_core_web_lg"

[corpora]

[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths.dev}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

[training]
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
dev_corpus = "corpora.dev"
train_corpus = "corpora.train"
before_to_disk = null
before_update = null

[training.batcher]
@batchers = "spacy.batch_by_words.v1"
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = "compounding.v1"
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = "spacy.ConsoleLogger.v1"
progress_bar = false

[training.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]